
==> Audit <==
|---------|-------------------------|----------|-----------------------|---------|---------------------|---------------------|
| Command |          Args           | Profile  |         User          | Version |     Start Time      |      End Time       |
|---------|-------------------------|----------|-----------------------|---------|---------------------|---------------------|
| start   | --driver=docker --force | minikube | DESKTOP-QGHJ1FC\admin | v1.35.0 | 01 May 25 16:39 IST | 01 May 25 16:45 IST |
| service | springboot-service      | minikube | DESKTOP-QGHJ1FC\admin | v1.35.0 | 01 May 25 17:33 IST |                     |
| service | springboot-service      | minikube | DESKTOP-QGHJ1FC\admin | v1.35.0 | 01 May 25 17:38 IST |                     |
| start   | --driver=docker         | minikube | DESKTOP-QGHJ1FC\admin | v1.35.0 | 01 May 25 17:39 IST | 01 May 25 17:41 IST |
| service | springboot-service      | minikube | DESKTOP-QGHJ1FC\admin | v1.35.0 | 01 May 25 17:41 IST |                     |
| service | springboot-service      | minikube | DESKTOP-QGHJ1FC\admin | v1.35.0 | 01 May 25 17:43 IST |                     |
| start   | --driver=docker         | minikube | DESKTOP-QGHJ1FC\admin | v1.35.0 | 01 May 25 17:44 IST | 01 May 25 17:45 IST |
| service | springboot-service      | minikube | DESKTOP-QGHJ1FC\admin | v1.35.0 | 01 May 25 17:45 IST |                     |
| service | springboot-service      | minikube | DESKTOP-QGHJ1FC\admin | v1.35.0 | 01 May 25 18:00 IST |                     |
|---------|-------------------------|----------|-----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/05/01 17:44:08
Running on machine: DESKTOP-QGHJ1FC
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0501 17:44:08.063849   27344 out.go:345] Setting OutFile to fd 84 ...
I0501 17:44:08.065851   27344 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0501 17:44:08.065851   27344 out.go:358] Setting ErrFile to fd 88...
I0501 17:44:08.065851   27344 out.go:392] TERM=,COLORTERM=, which probably does not support color
W0501 17:44:08.106801   27344 root.go:314] Error reading config file at C:\Users\admin\.minikube\config\config.json: open C:\Users\admin\.minikube\config\config.json: The system cannot find the file specified.
I0501 17:44:08.118331   27344 out.go:352] Setting JSON to false
I0501 17:44:08.130139   27344 start.go:129] hostinfo: {"hostname":"DESKTOP-QGHJ1FC","uptime":27609,"bootTime":1746074038,"procs":283,"os":"windows","platform":"Microsoft Windows 10 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.4529 Build 19045.4529","kernelVersion":"10.0.19045.4529 Build 19045.4529","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"46a51157-b58b-48cf-95fd-f83ae5f24db8"}
W0501 17:44:08.130139   27344 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0501 17:44:08.142758   27344 out.go:177] * minikube v1.35.0 on Microsoft Windows 10 Home Single Language 10.0.19045.4529 Build 19045.4529
I0501 17:44:08.163717   27344 notify.go:220] Checking for updates...
I0501 17:44:08.164717   27344 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0501 17:44:08.171863   27344 driver.go:394] Setting default libvirt URI to qemu:///system
I0501 17:44:08.761367   27344 docker.go:123] docker version: linux-24.0.5:Docker Desktop 4.22.0 (117440)
I0501 17:44:08.791939   27344 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0501 17:44:10.492666   27344 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.7007262s)
I0501 17:44:10.497129   27344 info.go:266] docker info: {ID:acde38cd-be67-4fd9-a788-81bc21d263bf Containers:13 ContainersRunning:1 ContainersPaused:0 ContainersStopped:12 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:77 OomKillDisable:true NGoroutines:134 SystemTime:2025-05-01 12:14:10.4208286 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:43 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6580133888 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0501 17:44:10.508127   27344 out.go:177] * Using the docker driver based on existing profile
I0501 17:44:10.534956   27344 start.go:297] selected driver: docker
I0501 17:44:10.534956   27344 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0501 17:44:10.534956   27344 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0501 17:44:10.595954   27344 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0501 17:44:16.510401   27344 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (5.9144471s)
I0501 17:44:16.510401   27344 info.go:266] docker info: {ID:acde38cd-be67-4fd9-a788-81bc21d263bf Containers:13 ContainersRunning:1 ContainersPaused:0 ContainersStopped:12 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:77 OomKillDisable:true NGoroutines:134 SystemTime:2025-05-01 12:14:16.4364889 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:43 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6580133888 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0501 17:44:16.795799   27344 cni.go:84] Creating CNI manager for ""
I0501 17:44:16.797785   27344 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0501 17:44:16.798785   27344 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0501 17:44:16.816159   27344 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0501 17:44:16.839998   27344 cache.go:121] Beginning downloading kic base image for docker with docker
I0501 17:44:16.859767   27344 out.go:177] * Pulling base image v0.0.46 ...
I0501 17:44:16.884314   27344 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0501 17:44:16.885413   27344 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0501 17:44:16.886436   27344 preload.go:146] Found local preload: C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0501 17:44:16.886436   27344 cache.go:56] Caching tarball of preloaded images
I0501 17:44:16.888478   27344 preload.go:172] Found C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0501 17:44:16.888478   27344 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0501 17:44:16.889480   27344 profile.go:143] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I0501 17:44:17.436574   27344 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0501 17:44:17.436574   27344 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0501 17:44:17.437572   27344 cache.go:227] Successfully downloaded all kic artifacts
I0501 17:44:17.438577   27344 start.go:360] acquireMachinesLock for minikube: {Name:mk643f7e9da83601d587a510a4be0c611c47f2bc Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0501 17:44:17.438577   27344 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0501 17:44:17.438577   27344 start.go:96] Skipping create...Using existing machine configuration
I0501 17:44:17.439575   27344 fix.go:54] fixHost starting: 
I0501 17:44:17.507573   27344 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0501 17:44:17.965551   27344 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0501 17:44:17.965551   27344 fix.go:138] unexpected machine state, will restart: <nil>
I0501 17:44:17.987665   27344 out.go:177] * Restarting existing docker container for "minikube" ...
I0501 17:44:18.036210   27344 cli_runner.go:164] Run: docker start minikube
I0501 17:44:21.996434   27344 cli_runner.go:217] Completed: docker start minikube: (3.9602241s)
I0501 17:44:22.049533   27344 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0501 17:44:22.942178   27344 kic.go:430] container "minikube" state is running.
I0501 17:44:22.997486   27344 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0501 17:44:24.037019   27344 cli_runner.go:217] Completed: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube: (1.0395336s)
I0501 17:44:24.037019   27344 profile.go:143] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I0501 17:44:24.041019   27344 machine.go:93] provisionDockerMachine start ...
I0501 17:44:24.080429   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:24.736861   27344 main.go:141] libmachine: Using SSH client type: native
I0501 17:44:24.785889   27344 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1165360] 0x1167ea0 <nil>  [] 0s} 127.0.0.1 62190 <nil> <nil>}
I0501 17:44:24.785889   27344 main.go:141] libmachine: About to run SSH command:
hostname
I0501 17:44:25.012263   27344 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0501 17:44:28.416751   27344 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0501 17:44:28.417738   27344 ubuntu.go:169] provisioning hostname "minikube"
I0501 17:44:28.453747   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:29.008249   27344 main.go:141] libmachine: Using SSH client type: native
I0501 17:44:29.008822   27344 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1165360] 0x1167ea0 <nil>  [] 0s} 127.0.0.1 62190 <nil> <nil>}
I0501 17:44:29.008822   27344 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0501 17:44:29.423993   27344 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0501 17:44:29.460579   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:29.969340   27344 main.go:141] libmachine: Using SSH client type: native
I0501 17:44:29.969340   27344 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1165360] 0x1167ea0 <nil>  [] 0s} 127.0.0.1 62190 <nil> <nil>}
I0501 17:44:29.969340   27344 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0501 17:44:30.314533   27344 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0501 17:44:30.316499   27344 ubuntu.go:175] set auth options {CertDir:C:\Users\admin\.minikube CaCertPath:C:\Users\admin\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\admin\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\admin\.minikube\machines\server.pem ServerKeyPath:C:\Users\admin\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\admin\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\admin\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\admin\.minikube}
I0501 17:44:30.317473   27344 ubuntu.go:177] setting up certificates
I0501 17:44:30.317473   27344 provision.go:84] configureAuth start
I0501 17:44:30.350713   27344 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0501 17:44:30.957926   27344 provision.go:143] copyHostCerts
I0501 17:44:30.962926   27344 exec_runner.go:144] found C:\Users\admin\.minikube/ca.pem, removing ...
I0501 17:44:30.963930   27344 exec_runner.go:203] rm: C:\Users\admin\.minikube\ca.pem
I0501 17:44:30.966931   27344 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\ca.pem --> C:\Users\admin\.minikube/ca.pem (1074 bytes)
I0501 17:44:30.971926   27344 exec_runner.go:144] found C:\Users\admin\.minikube/cert.pem, removing ...
I0501 17:44:30.971926   27344 exec_runner.go:203] rm: C:\Users\admin\.minikube\cert.pem
I0501 17:44:30.971926   27344 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\cert.pem --> C:\Users\admin\.minikube/cert.pem (1119 bytes)
I0501 17:44:30.983933   27344 exec_runner.go:144] found C:\Users\admin\.minikube/key.pem, removing ...
I0501 17:44:30.983933   27344 exec_runner.go:203] rm: C:\Users\admin\.minikube\key.pem
I0501 17:44:30.984930   27344 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\key.pem --> C:\Users\admin\.minikube/key.pem (1679 bytes)
I0501 17:44:30.987572   27344 provision.go:117] generating server cert: C:\Users\admin\.minikube\machines\server.pem ca-key=C:\Users\admin\.minikube\certs\ca.pem private-key=C:\Users\admin\.minikube\certs\ca-key.pem org=admin.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0501 17:44:32.690431   27344 provision.go:177] copyRemoteCerts
I0501 17:44:32.744721   27344 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0501 17:44:32.816956   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:33.419832   27344 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62190 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0501 17:44:33.728461   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0501 17:44:33.878222   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0501 17:44:34.014464   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0501 17:44:34.162807   27344 provision.go:87] duration metric: took 3.8453337s to configureAuth
I0501 17:44:34.162807   27344 ubuntu.go:193] setting minikube options for container-runtime
I0501 17:44:34.164778   27344 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0501 17:44:34.195292   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:34.675866   27344 main.go:141] libmachine: Using SSH client type: native
I0501 17:44:34.676632   27344 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1165360] 0x1167ea0 <nil>  [] 0s} 127.0.0.1 62190 <nil> <nil>}
I0501 17:44:34.676632   27344 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0501 17:44:35.039468   27344 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0501 17:44:35.039468   27344 ubuntu.go:71] root file system type: overlay
I0501 17:44:35.039468   27344 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0501 17:44:35.095769   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:35.619724   27344 main.go:141] libmachine: Using SSH client type: native
I0501 17:44:35.620787   27344 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1165360] 0x1167ea0 <nil>  [] 0s} 127.0.0.1 62190 <nil> <nil>}
I0501 17:44:35.620984   27344 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0501 17:44:36.011156   27344 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0501 17:44:36.048047   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:36.545368   27344 main.go:141] libmachine: Using SSH client type: native
I0501 17:44:36.546025   27344 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1165360] 0x1167ea0 <nil>  [] 0s} 127.0.0.1 62190 <nil> <nil>}
I0501 17:44:36.546025   27344 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0501 17:44:36.901328   27344 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0501 17:44:36.901328   27344 machine.go:96] duration metric: took 12.8603093s to provisionDockerMachine
I0501 17:44:36.901328   27344 start.go:293] postStartSetup for "minikube" (driver="docker")
I0501 17:44:36.901328   27344 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0501 17:44:36.967053   27344 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0501 17:44:37.003848   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:37.450639   27344 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62190 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0501 17:44:38.050195   27344 ssh_runner.go:195] Run: cat /etc/os-release
I0501 17:44:38.133160   27344 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0501 17:44:38.134163   27344 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0501 17:44:38.134163   27344 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0501 17:44:38.134163   27344 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0501 17:44:38.134163   27344 filesync.go:126] Scanning C:\Users\admin\.minikube\addons for local assets ...
I0501 17:44:38.163437   27344 filesync.go:126] Scanning C:\Users\admin\.minikube\files for local assets ...
I0501 17:44:38.164520   27344 start.go:296] duration metric: took 1.2631921s for postStartSetup
I0501 17:44:38.237332   27344 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0501 17:44:38.303175   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:39.241014   27344 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62190 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0501 17:44:39.448084   27344 ssh_runner.go:235] Completed: sh -c "df -h /var | awk 'NR==2{print $5}'": (1.2102359s)
I0501 17:44:39.556807   27344 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0501 17:44:39.582822   27344 fix.go:56] duration metric: took 22.1432475s for fixHost
I0501 17:44:39.582822   27344 start.go:83] releasing machines lock for "minikube", held for 22.1442453s
I0501 17:44:39.629506   27344 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0501 17:44:40.170124   27344 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0501 17:44:40.202706   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:40.213088   27344 ssh_runner.go:195] Run: cat /version.json
I0501 17:44:40.245612   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:44:41.011865   27344 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62190 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0501 17:44:41.077480   27344 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62190 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0501 17:44:41.272313   27344 ssh_runner.go:235] Completed: curl.exe -sS -m 2 https://registry.k8s.io/: (1.1021892s)
W0501 17:44:41.272313   27344 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0501 17:44:41.313927   27344 ssh_runner.go:235] Completed: cat /version.json: (1.1008148s)
I0501 17:44:41.380625   27344 ssh_runner.go:195] Run: systemctl --version
I0501 17:44:41.476461   27344 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0501 17:44:41.554203   27344 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0501 17:44:41.616986   27344 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
W0501 17:44:41.622399   27344 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0501 17:44:41.632005   27344 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0501 17:44:41.675014   27344 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0501 17:44:41.931925   27344 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0501 17:44:41.931925   27344 start.go:495] detecting cgroup driver to use...
I0501 17:44:41.931925   27344 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0501 17:44:41.957921   27344 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0501 17:44:42.096186   27344 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0501 17:44:42.231916   27344 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0501 17:44:42.290453   27344 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0501 17:44:42.349333   27344 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0501 17:44:42.474772   27344 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0501 17:44:42.594618   27344 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0501 17:44:42.691533   27344 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0501 17:44:42.807960   27344 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0501 17:44:42.921817   27344 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0501 17:44:43.066011   27344 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0501 17:44:43.263711   27344 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0501 17:44:43.373733   27344 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0501 17:44:43.474649   27344 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0501 17:44:43.598075   27344 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0501 17:44:44.259218   27344 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0501 17:44:44.770433   27344 start.go:495] detecting cgroup driver to use...
I0501 17:44:44.770495   27344 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0501 17:44:44.832703   27344 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0501 17:44:44.957923   27344 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0501 17:44:45.037051   27344 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0501 17:44:45.146817   27344 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0501 17:44:45.345932   27344 ssh_runner.go:195] Run: which cri-dockerd
I0501 17:44:45.461635   27344 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0501 17:44:45.533447   27344 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0501 17:44:45.721898   27344 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0501 17:44:46.437015   27344 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0501 17:44:47.020319   27344 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0501 17:44:47.027320   27344 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0501 17:44:47.184977   27344 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0501 17:44:47.858283   27344 ssh_runner.go:195] Run: sudo systemctl restart docker
I0501 17:44:55.556122   27344 ssh_runner.go:235] Completed: sudo systemctl restart docker: (7.6978389s)
I0501 17:44:55.618494   27344 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0501 17:44:55.736042   27344 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0501 17:44:55.857589   27344 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0501 17:44:55.985854   27344 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0501 17:44:56.549855   27344 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0501 17:44:57.089853   27344 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0501 17:44:57.672279   27344 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0501 17:44:57.790462   27344 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0501 17:44:57.910047   27344 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0501 17:44:58.480439   27344 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0501 17:44:58.883296   27344 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0501 17:44:58.945483   27344 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0501 17:44:58.969711   27344 start.go:563] Will wait 60s for crictl version
I0501 17:44:59.020619   27344 ssh_runner.go:195] Run: which crictl
I0501 17:44:59.087314   27344 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0501 17:44:59.310904   27344 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0501 17:44:59.344903   27344 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0501 17:44:59.508715   27344 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0501 17:44:59.657544   27344 out.go:235] * Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0501 17:44:59.704416   27344 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0501 17:45:00.485650   27344 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0501 17:45:00.541854   27344 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0501 17:45:00.561895   27344 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0501 17:45:00.668439   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0501 17:45:01.125810   27344 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0501 17:45:01.125810   27344 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0501 17:45:01.157808   27344 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0501 17:45:01.266340   27344 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0501 17:45:01.266340   27344 docker.go:619] Images already preloaded, skipping extraction
I0501 17:45:01.298649   27344 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0501 17:45:01.438202   27344 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0501 17:45:01.438202   27344 cache_images.go:84] Images are preloaded, skipping loading
I0501 17:45:01.438202   27344 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0501 17:45:01.444201   27344 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0501 17:45:01.475200   27344 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0501 17:45:01.732399   27344 cni.go:84] Creating CNI manager for ""
I0501 17:45:01.732399   27344 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0501 17:45:01.732399   27344 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0501 17:45:01.732399   27344 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0501 17:45:01.734645   27344 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0501 17:45:01.789598   27344 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0501 17:45:01.840256   27344 binaries.go:44] Found k8s binaries, skipping transfer
I0501 17:45:01.892758   27344 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0501 17:45:01.940407   27344 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0501 17:45:02.047503   27344 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0501 17:45:02.150755   27344 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0501 17:45:02.320625   27344 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0501 17:45:02.346026   27344 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0501 17:45:02.476778   27344 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0501 17:45:03.205851   27344 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0501 17:45:03.275649   27344 certs.go:68] Setting up C:\Users\admin\.minikube\profiles\minikube for IP: 192.168.49.2
I0501 17:45:03.275649   27344 certs.go:194] generating shared ca certs ...
I0501 17:45:03.276177   27344 certs.go:226] acquiring lock for ca certs: {Name:mkfd7a320f78a704b321a6e757eb4483941c4372 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0501 17:45:03.276961   27344 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\admin\.minikube\ca.key
I0501 17:45:03.278459   27344 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\admin\.minikube\proxy-client-ca.key
I0501 17:45:03.278459   27344 certs.go:256] generating profile certs ...
I0501 17:45:03.281269   27344 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\admin\.minikube\profiles\minikube\client.key
I0501 17:45:03.283249   27344 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\admin\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0501 17:45:03.285784   27344 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\admin\.minikube\profiles\minikube\proxy-client.key
I0501 17:45:03.287784   27344 certs.go:484] found cert: C:\Users\admin\.minikube\certs\ca-key.pem (1679 bytes)
I0501 17:45:03.287784   27344 certs.go:484] found cert: C:\Users\admin\.minikube\certs\ca.pem (1074 bytes)
I0501 17:45:03.288785   27344 certs.go:484] found cert: C:\Users\admin\.minikube\certs\cert.pem (1119 bytes)
I0501 17:45:03.288785   27344 certs.go:484] found cert: C:\Users\admin\.minikube\certs\key.pem (1679 bytes)
I0501 17:45:03.295160   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0501 17:45:03.470829   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0501 17:45:03.638115   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0501 17:45:03.809629   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0501 17:45:03.974632   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0501 17:45:04.158777   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0501 17:45:04.339182   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0501 17:45:04.748393   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0501 17:45:04.975395   27344 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0501 17:45:05.198497   27344 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0501 17:45:05.573496   27344 ssh_runner.go:195] Run: openssl version
I0501 17:45:05.733243   27344 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0501 17:45:06.027576   27344 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0501 17:45:06.069096   27344 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  1 11:13 /usr/share/ca-certificates/minikubeCA.pem
I0501 17:45:06.127172   27344 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0501 17:45:06.286066   27344 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0501 17:45:06.464334   27344 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0501 17:45:06.542622   27344 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0501 17:45:06.653175   27344 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0501 17:45:06.766585   27344 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0501 17:45:06.878411   27344 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0501 17:45:07.001765   27344 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0501 17:45:07.147143   27344 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0501 17:45:07.192334   27344 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0501 17:45:07.226727   27344 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0501 17:45:07.429650   27344 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0501 17:45:07.485904   27344 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0501 17:45:07.486021   27344 kubeadm.go:593] restartPrimaryControlPlane start ...
I0501 17:45:07.544595   27344 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0501 17:45:07.602554   27344 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0501 17:45:07.640158   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0501 17:45:08.314562   27344 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:61229"
I0501 17:45:08.314698   27344 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:61229, want: 127.0.0.1:62194
I0501 17:45:08.315943   27344 kubeconfig.go:62] C:\Users\admin\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I0501 17:45:08.318388   27344 lock.go:35] WriteFile acquiring C:\Users\admin\.kube\config: {Name:mk0f6f650be7ab5b3e1ae83f14563cf1eb0bd30b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0501 17:45:08.593888   27344 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0501 17:45:08.786605   27344 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0501 17:45:08.787598   27344 kubeadm.go:597] duration metric: took 1.3015768s to restartPrimaryControlPlane
I0501 17:45:08.787598   27344 kubeadm.go:394] duration metric: took 1.5952643s to StartCluster
I0501 17:45:08.789241   27344 settings.go:142] acquiring lock: {Name:mk5c419c3fabff3b09a504acb297c33a24a93937 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0501 17:45:08.789241   27344 settings.go:150] Updating kubeconfig:  C:\Users\admin\.kube\config
I0501 17:45:08.811557   27344 lock.go:35] WriteFile acquiring C:\Users\admin\.kube\config: {Name:mk0f6f650be7ab5b3e1ae83f14563cf1eb0bd30b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0501 17:45:08.814559   27344 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0501 17:45:08.815556   27344 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0501 17:45:08.815556   27344 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0501 17:45:08.817558   27344 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0501 17:45:08.817558   27344 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0501 17:45:08.826830   27344 out.go:177] * Verifying Kubernetes components...
I0501 17:45:08.828419   27344 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0501 17:45:08.828419   27344 addons.go:247] addon storage-provisioner should already be in state true
I0501 17:45:08.828419   27344 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0501 17:45:08.865844   27344 host.go:66] Checking if "minikube" exists ...
I0501 17:45:08.922512   27344 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0501 17:45:08.947134   27344 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0501 17:45:08.958222   27344 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0501 17:45:09.579389   27344 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0501 17:45:09.592602   27344 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0501 17:45:09.592602   27344 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0501 17:45:09.597614   27344 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0501 17:45:09.597614   27344 addons.go:247] addon default-storageclass should already be in state true
I0501 17:45:09.598610   27344 host.go:66] Checking if "minikube" exists ...
I0501 17:45:09.627003   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:45:09.689686   27344 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0501 17:45:10.170404   27344 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.247892s)
I0501 17:45:10.244818   27344 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0501 17:45:10.293440   27344 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62190 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0501 17:45:10.412815   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0501 17:45:10.486590   27344 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0501 17:45:10.486590   27344 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0501 17:45:10.524077   27344 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0501 17:45:11.064392   27344 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0501 17:45:11.225228   27344 api_server.go:52] waiting for apiserver process to appear ...
I0501 17:45:11.289837   27344 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62190 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0501 17:45:11.308594   27344 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0501 17:45:12.277312   27344 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0501 17:45:36.069839   27344 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (25.0054467s)
I0501 17:45:36.070840   27344 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (24.7622457s)
I0501 17:45:36.070840   27344 api_server.go:72] duration metric: took 27.2552842s to wait for apiserver process to appear ...
I0501 17:45:36.070840   27344 api_server.go:88] waiting for apiserver healthz status ...
I0501 17:45:36.070840   27344 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62194/healthz ...
I0501 17:45:36.071848   27344 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (23.794536s)
I0501 17:45:36.242272   27344 api_server.go:279] https://127.0.0.1:62194/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0501 17:45:36.242272   27344 api_server.go:103] status: https://127.0.0.1:62194/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0501 17:45:36.572201   27344 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62194/healthz ...
I0501 17:45:36.588118   27344 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0501 17:45:36.609953   27344 addons.go:514] duration metric: took 27.7943974s for enable addons: enabled=[storage-provisioner default-storageclass]
I0501 17:45:36.689864   27344 api_server.go:279] https://127.0.0.1:62194/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0501 17:45:36.689864   27344 api_server.go:103] status: https://127.0.0.1:62194/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0501 17:45:37.071199   27344 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62194/healthz ...
I0501 17:45:37.153005   27344 api_server.go:279] https://127.0.0.1:62194/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0501 17:45:37.153005   27344 api_server.go:103] status: https://127.0.0.1:62194/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0501 17:45:37.571620   27344 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62194/healthz ...
I0501 17:45:37.657458   27344 api_server.go:279] https://127.0.0.1:62194/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0501 17:45:37.657458   27344 api_server.go:103] status: https://127.0.0.1:62194/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0501 17:45:38.071796   27344 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62194/healthz ...
I0501 17:45:38.166122   27344 api_server.go:279] https://127.0.0.1:62194/healthz returned 200:
ok
I0501 17:45:38.197296   27344 api_server.go:141] control plane version: v1.32.0
I0501 17:45:38.197296   27344 api_server.go:131] duration metric: took 2.1264562s to wait for apiserver health ...
I0501 17:45:38.197296   27344 system_pods.go:43] waiting for kube-system pods to appear ...
I0501 17:45:38.345298   27344 system_pods.go:59] 7 kube-system pods found
I0501 17:45:38.345298   27344 system_pods.go:61] "coredns-668d6bf9bc-xdm24" [64ae9708-c150-45df-8e73-0d1605c8697a] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0501 17:45:38.345298   27344 system_pods.go:61] "etcd-minikube" [92066f86-2ff4-415e-8c1e-ad80a2e5abec] Running
I0501 17:45:38.345298   27344 system_pods.go:61] "kube-apiserver-minikube" [1048133a-9f2e-4b7b-8cca-f8b35fc85c35] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0501 17:45:38.345298   27344 system_pods.go:61] "kube-controller-manager-minikube" [6c3b65eb-4330-4af4-a65f-c4350b4880b0] Running
I0501 17:45:38.345809   27344 system_pods.go:61] "kube-proxy-5ks2g" [d2597315-6873-4ded-8844-8ffacc36d426] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0501 17:45:38.345809   27344 system_pods.go:61] "kube-scheduler-minikube" [dff7b244-979a-48b9-990a-9982e108189f] Running
I0501 17:45:38.345843   27344 system_pods.go:61] "storage-provisioner" [ec96c2f6-2038-4013-85bc-d8118635fdee] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0501 17:45:38.345843   27344 system_pods.go:74] duration metric: took 148.5467ms to wait for pod list to return data ...
I0501 17:45:38.345865   27344 kubeadm.go:582] duration metric: took 29.5303093s to wait for: map[apiserver:true system_pods:true]
I0501 17:45:38.345865   27344 node_conditions.go:102] verifying NodePressure condition ...
I0501 17:45:38.511155   27344 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0501 17:45:38.511155   27344 node_conditions.go:123] node cpu capacity is 4
I0501 17:45:38.511155   27344 node_conditions.go:105] duration metric: took 165.2902ms to run NodePressure ...
I0501 17:45:38.511155   27344 start.go:241] waiting for startup goroutines ...
I0501 17:45:38.511155   27344 start.go:246] waiting for cluster config update ...
I0501 17:45:38.511155   27344 start.go:255] writing updated cluster config ...
I0501 17:45:38.588428   27344 ssh_runner.go:195] Run: rm -f paused
I0501 17:45:39.150980   27344 start.go:600] kubectl: 1.27.2, cluster: 1.32.0 (minor skew: 5)
I0501 17:45:39.161979   27344 out.go:201] 
W0501 17:45:39.176620   27344 out.go:270] ! C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.27.2, which may have incompatibilities with Kubernetes 1.32.0.
I0501 17:45:39.201644   27344 out.go:177]   - Want kubectl v1.32.0? Try 'minikube kubectl -- get pods -A'
I0501 17:45:39.269115   27344 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 01 12:14:50 minikube systemd[1]: Starting Docker Application Container Engine...
May 01 12:14:50 minikube dockerd[1101]: time="2025-05-01T12:14:50.624983800Z" level=info msg="Starting up"
May 01 12:14:50 minikube dockerd[1101]: time="2025-05-01T12:14:50.636100200Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
May 01 12:14:50 minikube dockerd[1101]: time="2025-05-01T12:14:50.782027600Z" level=info msg="[graphdriver] trying configured driver: overlay2"
May 01 12:14:50 minikube dockerd[1101]: time="2025-05-01T12:14:50.943905700Z" level=info msg="Loading containers: start."
May 01 12:14:54 minikube dockerd[1101]: time="2025-05-01T12:14:54.039106600Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
May 01 12:14:55 minikube dockerd[1101]: time="2025-05-01T12:14:55.321409000Z" level=info msg="Loading containers: done."
May 01 12:14:55 minikube dockerd[1101]: time="2025-05-01T12:14:55.414691100Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
May 01 12:14:55 minikube dockerd[1101]: time="2025-05-01T12:14:55.414815700Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
May 01 12:14:55 minikube dockerd[1101]: time="2025-05-01T12:14:55.414842300Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
May 01 12:14:55 minikube dockerd[1101]: time="2025-05-01T12:14:55.414868700Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
May 01 12:14:55 minikube dockerd[1101]: time="2025-05-01T12:14:55.414915700Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
May 01 12:14:55 minikube dockerd[1101]: time="2025-05-01T12:14:55.415036300Z" level=info msg="Daemon has completed initialization"
May 01 12:14:55 minikube dockerd[1101]: time="2025-05-01T12:14:55.555632000Z" level=info msg="API listen on /var/run/docker.sock"
May 01 12:14:55 minikube dockerd[1101]: time="2025-05-01T12:14:55.555649200Z" level=info msg="API listen on [::]:2376"
May 01 12:14:55 minikube systemd[1]: Started Docker Application Container Engine.
May 01 12:14:58 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 01 12:14:58 minikube cri-dockerd[1404]: time="2025-05-01T12:14:58Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 01 12:14:58 minikube cri-dockerd[1404]: time="2025-05-01T12:14:58Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 01 12:14:58 minikube cri-dockerd[1404]: time="2025-05-01T12:14:58Z" level=info msg="Start docker client with request timeout 0s"
May 01 12:14:58 minikube cri-dockerd[1404]: time="2025-05-01T12:14:58Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 01 12:14:58 minikube cri-dockerd[1404]: time="2025-05-01T12:14:58Z" level=info msg="Loaded network plugin cni"
May 01 12:14:58 minikube cri-dockerd[1404]: time="2025-05-01T12:14:58Z" level=info msg="Docker cri networking managed by network plugin cni"
May 01 12:14:58 minikube cri-dockerd[1404]: time="2025-05-01T12:14:58Z" level=info msg="Setting cgroupDriver cgroupfs"
May 01 12:14:58 minikube cri-dockerd[1404]: time="2025-05-01T12:14:58Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 01 12:14:58 minikube cri-dockerd[1404]: time="2025-05-01T12:14:58Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 01 12:14:58 minikube cri-dockerd[1404]: time="2025-05-01T12:14:58Z" level=info msg="Start cri-dockerd grpc backend"
May 01 12:14:58 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 01 12:15:05 minikube cri-dockerd[1404]: time="2025-05-01T12:15:05Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-xdm24_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5a9c9f631225ddd834b4e03528f398b2a9e0d26f7b838a16a6c82cefdd1c1dde\""
May 01 12:15:05 minikube cri-dockerd[1404]: time="2025-05-01T12:15:05Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"springboot-app-6f8c5d596d-kmwjm_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ea4ae6d08622e9b53b074789ea236f511b030936488468e4fe679034019fdb8f\""
May 01 12:15:08 minikube cri-dockerd[1404]: time="2025-05-01T12:15:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bf9bf378a8879fbf2e5c352964c6aa650c325e0d7a530a2f23d514ce61298193/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 01 12:15:10 minikube cri-dockerd[1404]: time="2025-05-01T12:15:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/73d4e7baed1ec41ca7d6e40389e21402536c279969ca2f2213b969bafab12e52/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 01 12:15:10 minikube cri-dockerd[1404]: time="2025-05-01T12:15:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d602eecc8d8293d518641572ffe792b6afa84a6e08fd20078ec8e76c174abd9c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 01 12:15:10 minikube cri-dockerd[1404]: time="2025-05-01T12:15:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a7b375191827fb670ad6bf928b84001392550ef0017702d8392e13536273cc5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 01 12:15:35 minikube cri-dockerd[1404]: time="2025-05-01T12:15:35Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 01 12:15:35 minikube cri-dockerd[1404]: time="2025-05-01T12:15:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cfb520b2fbabc94bea39b07847c0439ba288a8793d923a8a113eb5c2bf73c4c5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 01 12:15:37 minikube cri-dockerd[1404]: time="2025-05-01T12:15:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/22d860a652af661f79c2eb41970b02509dc948f1ff8243b3a02f2eeed4e0fe89/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 01 12:15:37 minikube cri-dockerd[1404]: time="2025-05-01T12:15:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/438703130e7b9254307aa0b355713e2fedfd1319a823dec952ee15c0a9c0fc47/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 01 12:15:38 minikube cri-dockerd[1404]: time="2025-05-01T12:15:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f65ddfee01fa2bd1c93e129911d2e08c50c0d9741c9e737f4c862004ce0526fa/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 01 12:15:44 minikube dockerd[1101]: time="2025-05-01T12:15:44.723485300Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 01 12:15:44 minikube dockerd[1101]: time="2025-05-01T12:15:44.723565600Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 01 12:15:50 minikube dockerd[1101]: time="2025-05-01T12:15:50.596746100Z" level=info msg="ignoring event" container=223d68e980011a9d652bdbc26211030a310942cb94099699a71ed8c87fa60338 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 01 12:16:00 minikube dockerd[1101]: time="2025-05-01T12:16:00.162083700Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 01 12:16:00 minikube dockerd[1101]: time="2025-05-01T12:16:00.162267400Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 01 12:16:30 minikube dockerd[1101]: time="2025-05-01T12:16:30.958561800Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 01 12:16:30 minikube dockerd[1101]: time="2025-05-01T12:16:30.958697200Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 01 12:17:16 minikube dockerd[1101]: time="2025-05-01T12:17:16.585478800Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 01 12:17:16 minikube dockerd[1101]: time="2025-05-01T12:17:16.585624700Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 01 12:18:48 minikube dockerd[1101]: time="2025-05-01T12:18:48.145662900Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 01 12:18:48 minikube dockerd[1101]: time="2025-05-01T12:18:48.145798500Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 01 12:21:36 minikube dockerd[1101]: time="2025-05-01T12:21:36.557274000Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 01 12:21:36 minikube dockerd[1101]: time="2025-05-01T12:21:36.557434000Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 01 12:26:52 minikube dockerd[1101]: time="2025-05-01T12:26:52.401106600Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 01 12:26:52 minikube dockerd[1101]: time="2025-05-01T12:26:52.401237600Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 01 12:31:57 minikube dockerd[1101]: time="2025-05-01T12:31:57.285307000Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 01 12:31:57 minikube dockerd[1101]: time="2025-05-01T12:31:57.285408000Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 01 12:37:06 minikube dockerd[1101]: time="2025-05-01T12:37:06.316916500Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 01 12:37:06 minikube dockerd[1101]: time="2025-05-01T12:37:06.317234800Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 01 12:42:17 minikube dockerd[1101]: time="2025-05-01T12:42:17.437041300Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 01 12:42:17 minikube dockerd[1101]: time="2025-05-01T12:42:17.437134600Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
c9dcd910fa3f8       6e38f40d628db       29 minutes ago      Running             storage-provisioner       5                   22d860a652af6       storage-provisioner
3c722cc187e17       c69fa2e9cbf5f       30 minutes ago      Running             coredns                   2                   438703130e7b9       coredns-668d6bf9bc-xdm24
223d68e980011       6e38f40d628db       30 minutes ago      Exited              storage-provisioner       4                   22d860a652af6       storage-provisioner
42c527f931889       040f9f8aac8cd       30 minutes ago      Running             kube-proxy                2                   cfb520b2fbabc       kube-proxy-5ks2g
d1ae22961895b       a389e107f4ff1       30 minutes ago      Running             kube-scheduler            2                   d602eecc8d829       kube-scheduler-minikube
a09582a8f6b7c       8cab3d2a8bd0f       30 minutes ago      Running             kube-controller-manager   3                   2a7b375191827       kube-controller-manager-minikube
e7daf90fb83fc       a9e7e6b294baf       30 minutes ago      Running             etcd                      2                   73d4e7baed1ec       etcd-minikube
6dbc24eecdd44       c2e17b8d0f4a3       30 minutes ago      Running             kube-apiserver            2                   bf9bf378a8879       kube-apiserver-minikube
66165f0d6f853       c69fa2e9cbf5f       35 minutes ago      Exited              coredns                   1                   5a9c9f631225d       coredns-668d6bf9bc-xdm24
4ad4c1e753b2b       a9e7e6b294baf       35 minutes ago      Exited              etcd                      1                   37aef35d9ea48       etcd-minikube
6f94f45464092       a389e107f4ff1       35 minutes ago      Exited              kube-scheduler            1                   3541aa3dc8289       kube-scheduler-minikube
7489582f365c6       8cab3d2a8bd0f       35 minutes ago      Exited              kube-controller-manager   2                   07c263bea2d55       kube-controller-manager-minikube
9da2cbb5a94d9       c2e17b8d0f4a3       35 minutes ago      Exited              kube-apiserver            1                   fe90ce0d5e5c2       kube-apiserver-minikube
0be714d647098       040f9f8aac8cd       35 minutes ago      Exited              kube-proxy                1                   cad2086c2a381       kube-proxy-5ks2g


==> coredns [3c722cc187e1] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[150639226]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (01-May-2025 12:15:42.894) (total time: 10019ms):
Trace[150639226]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10018ms (12:15:52.913)
Trace[150639226]: [10.0191355s] [10.0191355s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1959206429]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (01-May-2025 12:15:42.894) (total time: 10019ms):
Trace[1959206429]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10018ms (12:15:52.913)
Trace[1959206429]: [10.0196009s] [10.0196009s] END
[INFO] plugin/kubernetes: Trace[423575263]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (01-May-2025 12:15:42.895) (total time: 10018ms):
Trace[423575263]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10018ms (12:15:52.913)
Trace[423575263]: [10.0188245s] [10.0188245s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] 127.0.0.1:49685 - 58233 "HINFO IN 4458011423519512334.6469613740113468168. udp 57 false 512" - - 0 6.0034867s
[ERROR] plugin/errors: 2 4458011423519512334.6469613740113468168. HINFO: read udp 10.244.0.6:52328->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:35909 - 65253 "HINFO IN 4458011423519512334.6469613740113468168. udp 57 false 512" - - 0 2.0077828s
[ERROR] plugin/errors: 2 4458011423519512334.6469613740113468168. HINFO: read udp 10.244.0.6:54131->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:40429 - 663 "HINFO IN 4458011423519512334.6469613740113468168. udp 57 false 512" - - 0 6.0039282s
[ERROR] plugin/errors: 2 4458011423519512334.6469613740113468168. HINFO: read udp 10.244.0.6:45757->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:42000 - 46728 "HINFO IN 4458011423519512334.6469613740113468168. udp 57 false 512" - - 0 2.0006799s
[ERROR] plugin/errors: 2 4458011423519512334.6469613740113468168. HINFO: read udp 10.244.0.6:38654->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:56143 - 6352 "HINFO IN 4458011423519512334.6469613740113468168. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.1567761s


==> coredns [66165f0d6f85] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1142860301]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (01-May-2025 12:10:59.154) (total time: 10024ms):
Trace[1142860301]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10024ms (12:11:09.179)
Trace[1142860301]: [10.0248648s] [10.0248648s] END
[INFO] plugin/kubernetes: Trace[290371354]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (01-May-2025 12:10:59.157) (total time: 10018ms):
Trace[290371354]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10016ms (12:11:09.173)
Trace[290371354]: [10.0189912s] [10.0189912s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1180015844]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (01-May-2025 12:10:59.158) (total time: 10022ms):
Trace[1180015844]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10012ms (12:11:09.170)
Trace[1180015844]: [10.0222839s] [10.0222839s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] 127.0.0.1:58276 - 352 "HINFO IN 1793043899083743289.4270949272761805731. udp 57 false 512" - - 0 6.0363228s
[ERROR] plugin/errors: 2 1793043899083743289.4270949272761805731. HINFO: read udp 10.244.0.4:47369->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:35734 - 15668 "HINFO IN 1793043899083743289.4270949272761805731. udp 57 false 512" - - 0 6.0117109s
[ERROR] plugin/errors: 2 1793043899083743289.4270949272761805731. HINFO: read udp 10.244.0.4:32954->192.168.65.254:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:46138 - 63808 "HINFO IN 1793043899083743289.4270949272761805731. udp 57 false 512" - - 0 4.0165s
[ERROR] plugin/errors: 2 1793043899083743289.4270949272761805731. HINFO: read udp 10.244.0.4:59277->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:39797 - 60542 "HINFO IN 1793043899083743289.4270949272761805731. udp 57 false 512" - - 0 2.0570101s
[ERROR] plugin/errors: 2 1793043899083743289.4270949272761805731. HINFO: read udp 10.244.0.4:33077->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:47504 - 9737 "HINFO IN 1793043899083743289.4270949272761805731. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.2011767s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_01T16_44_52_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 01 May 2025 11:14:33 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 01 May 2025 12:45:47 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 01 May 2025 12:42:50 +0000   Thu, 01 May 2025 11:14:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 01 May 2025 12:42:50 +0000   Thu, 01 May 2025 11:14:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 01 May 2025 12:42:50 +0000   Thu, 01 May 2025 11:14:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 01 May 2025 12:42:50 +0000   Thu, 01 May 2025 11:14:34 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             6425912Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             6425912Ki
  pods:               110
System Info:
  Machine ID:                 9f0617e2a52944fb84d4a5a291915cd2
  System UUID:                9f0617e2a52944fb84d4a5a291915cd2
  Boot ID:                    19f72b71-1aa1-4695-b02a-a6391cde6d51
  Kernel Version:             5.10.16.3-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     springboot-app-6f8c5d596d-kmwjm     0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m
  kube-system                 coredns-668d6bf9bc-xdm24            100m (2%)     0 (0%)      70Mi (1%)        170Mi (2%)     90m
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         91m
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         91m
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         91m
  kube-system                 kube-proxy-5ks2g                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         90m
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         91m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         90m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           90m                kube-proxy       
  Normal   Starting                           30m                kube-proxy       
  Normal   Starting                           34m                kube-proxy       
  Normal   NodeHasSufficientMemory            91m (x8 over 91m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              91m (x8 over 91m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               91m (x7 over 91m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            91m                kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  91m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           91m                kubelet          Starting kubelet.
  Warning  CgroupV1                           91m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasNoDiskPressure              90m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientMemory            90m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasSufficientPID               90m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            90m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     90m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode                     34m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                           30m                kubelet          Starting kubelet.
  Warning  PossibleMemoryBackedVolumesOnDisk  30m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Warning  CgroupV1                           30m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            30m (x8 over 30m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              30m (x8 over 30m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               30m (x7 over 30m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            30m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     30m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May 1 10:19] WSL2: Performing memory compaction.
[May 1 10:20] WSL2: Performing memory compaction.
[May 1 10:22] WSL2: Performing memory compaction.
[May 1 10:23] WSL2: Performing memory compaction.
[May 1 10:24] WSL2: Performing memory compaction.
[May 1 10:25] WSL2: Performing memory compaction.
[May 1 10:26] WSL2: Performing memory compaction.
[May 1 10:27] WSL2: Performing memory compaction.
[May 1 10:28] WSL2: Performing memory compaction.
[May 1 10:29] WSL2: Performing memory compaction.
[May 1 10:30] WSL2: Performing memory compaction.
[May 1 10:31] WSL2: Performing memory compaction.
[May 1 10:32] WSL2: Performing memory compaction.
[May 1 10:33] WSL2: Performing memory compaction.
[May 1 10:34] WSL2: Performing memory compaction.
[May 1 10:35] WSL2: Performing memory compaction.
[May 1 10:36] WSL2: Performing memory compaction.
[May 1 10:37] WSL2: Performing memory compaction.
[May 1 10:38] WSL2: Performing memory compaction.
[May 1 10:39] WSL2: Performing memory compaction.
[May 1 10:43] WSL2: Performing memory compaction.
[May 1 10:44] WSL2: Performing memory compaction.
[May 1 10:46] WSL2: Performing memory compaction.
[May 1 10:47] WSL2: Performing memory compaction.
[May 1 10:48] WSL2: Performing memory compaction.
[May 1 10:49] WSL2: Performing memory compaction.
[May 1 10:51] WSL2: Performing memory compaction.
[May 1 10:52] WSL2: Performing memory compaction.
[May 1 10:53] WSL2: Performing memory compaction.
[May 1 10:54] WSL2: Performing memory compaction.
[May 1 10:55] WSL2: Performing memory compaction.
[May 1 10:56] WSL2: Performing memory compaction.
[May 1 10:57] WSL2: Performing memory compaction.
[May 1 10:58] WSL2: Performing memory compaction.
[May 1 10:59] WSL2: Performing memory compaction.
[May 1 11:00] WSL2: Performing memory compaction.
[May 1 11:01] WSL2: Performing memory compaction.
[May 1 11:02] WSL2: Performing memory compaction.
[May 1 11:03] WSL2: Performing memory compaction.
[May 1 11:04] WSL2: Performing memory compaction.
[May 1 11:06] WSL2: Performing memory compaction.
[May 1 11:07] WSL2: Performing memory compaction.
[May 1 11:08] WSL2: Performing memory compaction.
[May 1 11:09] WSL2: Performing memory compaction.
[May 1 11:10] WSL2: Performing memory compaction.
[May 1 11:11] WSL2: Performing memory compaction.
[May 1 11:12] WSL2: Performing memory compaction.
[May 1 11:14] tmpfs: Unknown parameter 'noswap'
[ +35.937645] tmpfs: Unknown parameter 'noswap'
[May 1 11:24] WSL2: Performing memory compaction.
[May 1 11:38] WSL2: Performing memory compaction.
[May 1 11:44] WSL2: Performing memory compaction.
[May 1 11:46] WSL2: Performing memory compaction.
[May 1 12:05] WSL2: Performing memory compaction.
[May 1 12:10] WSL2: Performing memory compaction.
[May 1 12:13] WSL2: Performing memory compaction.
[May 1 12:14] WSL2: Performing memory compaction.
[May 1 12:15] tmpfs: Unknown parameter 'noswap'
[May 1 12:17] WSL2: Performing memory compaction.
[May 1 12:20] WSL2: Performing memory compaction.


==> etcd [4ad4c1e753b2] <==
{"level":"info","ts":"2025-05-01T12:11:26.105974Z","caller":"traceutil/trace.go:171","msg":"trace[721678702] range","detail":"{range_begin:/registry/statefulsets; range_end:; response_count:0; response_revision:3231; }","duration":"102.2574ms","start":"2025-05-01T12:11:26.003706Z","end":"2025-05-01T12:11:26.105964Z","steps":["trace[721678702] 'range keys from in-memory index tree'  (duration: 101.9681ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.106144Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.3732ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:11:26.106178Z","caller":"traceutil/trace.go:171","msg":"trace[1025505383] range","detail":"{range_begin:/registry/roles; range_end:; response_count:0; response_revision:3231; }","duration":"102.4884ms","start":"2025-05-01T12:11:26.003679Z","end":"2025-05-01T12:11:26.106168Z","steps":["trace[1025505383] 'range keys from in-memory index tree'  (duration: 102.167ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.111968Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.2363ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:11:26.112059Z","caller":"traceutil/trace.go:171","msg":"trace[1171256665] range","detail":"{range_begin:/registry/persistentvolumes; range_end:; response_count:0; response_revision:3231; }","duration":"108.3735ms","start":"2025-05-01T12:11:26.003661Z","end":"2025-05-01T12:11:26.112034Z","steps":["trace[1171256665] 'range keys from in-memory index tree'  (duration: 107.9271ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.150434Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"146.7658ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:11:26.150520Z","caller":"traceutil/trace.go:171","msg":"trace[1295920161] range","detail":"{range_begin:/registry/namespaces; range_end:; response_count:0; response_revision:3231; }","duration":"146.8948ms","start":"2025-05-01T12:11:26.003603Z","end":"2025-05-01T12:11:26.150498Z","steps":["trace[1295920161] 'range keys from in-memory index tree'  (duration: 146.2766ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.150767Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"146.8643ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:11:26.150817Z","caller":"traceutil/trace.go:171","msg":"trace[996539335] range","detail":"{range_begin:/registry/replicasets; range_end:; response_count:0; response_revision:3231; }","duration":"146.9369ms","start":"2025-05-01T12:11:26.003865Z","end":"2025-05-01T12:11:26.150802Z","steps":["trace[996539335] 'range keys from in-memory index tree'  (duration: 146.7426ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.151009Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"147.1527ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllerrevisions\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:11:26.151055Z","caller":"traceutil/trace.go:171","msg":"trace[1751292472] range","detail":"{range_begin:/registry/controllerrevisions; range_end:; response_count:0; response_revision:3231; }","duration":"147.2211ms","start":"2025-05-01T12:11:26.003818Z","end":"2025-05-01T12:11:26.151039Z","steps":["trace[1751292472] 'range keys from in-memory index tree'  (duration: 146.9826ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.171384Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.7657ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:11:26.171770Z","caller":"traceutil/trace.go:171","msg":"trace[56350959] range","detail":"{range_begin:/registry/minions; range_end:; response_count:0; response_revision:3231; }","duration":"168.0033ms","start":"2025-05-01T12:11:26.003641Z","end":"2025-05-01T12:11:26.171645Z","steps":["trace[56350959] 'range keys from in-memory index tree'  (duration: 108.4324ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.374929Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"117.3871ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/storage-provisioner.183b660069e9f55c\" limit:1 ","response":"range_response_count:1 size:734"}
{"level":"info","ts":"2025-05-01T12:11:26.375191Z","caller":"traceutil/trace.go:171","msg":"trace[743863651] range","detail":"{range_begin:/registry/events/kube-system/storage-provisioner.183b660069e9f55c; range_end:; response_count:1; response_revision:3232; }","duration":"117.6561ms","start":"2025-05-01T12:11:26.257502Z","end":"2025-05-01T12:11:26.375158Z","steps":["trace[743863651] 'range keys from in-memory index tree'  (duration: 116.7905ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.452997Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"187.6627ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingadmissionpolicybindings\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:11:26.453202Z","caller":"traceutil/trace.go:171","msg":"trace[1320835303] range","detail":"{range_begin:/registry/validatingadmissionpolicybindings; range_end:; response_count:0; response_revision:3232; }","duration":"187.904ms","start":"2025-05-01T12:11:26.286788Z","end":"2025-05-01T12:11:26.453175Z","steps":["trace[1320835303] 'range keys from in-memory index tree'  (duration: 187.3511ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:11:26.630519Z","caller":"traceutil/trace.go:171","msg":"trace[951733409] transaction","detail":"{read_only:false; response_revision:3233; number_of_response:1; }","duration":"164.7737ms","start":"2025-05-01T12:11:26.465719Z","end":"2025-05-01T12:11:26.630492Z","steps":["trace[951733409] 'process raft request'  (duration: 164.6482ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.839086Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.6012ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036954493685321 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/minikube.183b660d008335f8\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/minikube.183b660d008335f8\" value_size:581 lease:8128036954493684858 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-05-01T12:11:26.839320Z","caller":"traceutil/trace.go:171","msg":"trace[908719979] linearizableReadLoop","detail":"{readStateIndex:3926; appliedIndex:3925; }","duration":"208.2708ms","start":"2025-05-01T12:11:26.631023Z","end":"2025-05-01T12:11:26.839293Z","steps":["trace[908719979] 'read index received'  (duration: 97.2938ms)","trace[908719979] 'applied index is now lower than readState.Index'  (duration: 110.9747ms)"],"step_count":2}
{"level":"info","ts":"2025-05-01T12:11:26.839560Z","caller":"traceutil/trace.go:171","msg":"trace[1073936965] transaction","detail":"{read_only:false; response_revision:3234; number_of_response:1; }","duration":"268.0872ms","start":"2025-05-01T12:11:26.571452Z","end":"2025-05-01T12:11:26.839539Z","steps":["trace[1073936965] 'process raft request'  (duration: 156.9663ms)","trace[1073936965] 'compare'  (duration: 110.3222ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-01T12:11:26.839879Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"285.2539ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/endpoint-controller\" limit:1 ","response":"range_response_count:1 size:203"}
{"level":"info","ts":"2025-05-01T12:11:26.839932Z","caller":"traceutil/trace.go:171","msg":"trace[1133293680] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/endpoint-controller; range_end:; response_count:1; response_revision:3234; }","duration":"285.3353ms","start":"2025-05-01T12:11:26.554582Z","end":"2025-05-01T12:11:26.839917Z","steps":["trace[1133293680] 'agreement among raft nodes before linearized reading'  (duration: 285.188ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.852367Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.4585ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/endpointslice-controller\" limit:1 ","response":"range_response_count:1 size:214"}
{"level":"info","ts":"2025-05-01T12:11:26.853467Z","caller":"traceutil/trace.go:171","msg":"trace[1056553251] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/endpointslice-controller; range_end:; response_count:1; response_revision:3235; }","duration":"102.5951ms","start":"2025-05-01T12:11:26.750844Z","end":"2025-05-01T12:11:26.853439Z","steps":["trace[1056553251] 'agreement among raft nodes before linearized reading'  (duration: 101.3607ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:11:26.855774Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.7041ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/legacy-service-account-token-cleaner\" limit:1 ","response":"range_response_count:1 size:238"}
{"level":"info","ts":"2025-05-01T12:11:26.865334Z","caller":"traceutil/trace.go:171","msg":"trace[399383456] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/legacy-service-account-token-cleaner; range_end:; response_count:1; response_revision:3235; }","duration":"114.3012ms","start":"2025-05-01T12:11:26.751003Z","end":"2025-05-01T12:11:26.865305Z","steps":["trace[399383456] 'agreement among raft nodes before linearized reading'  (duration: 104.4504ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:11:52.218028Z","caller":"traceutil/trace.go:171","msg":"trace[605590752] transaction","detail":"{read_only:false; response_revision:3290; number_of_response:1; }","duration":"124.1756ms","start":"2025-05-01T12:11:52.093822Z","end":"2025-05-01T12:11:52.217998Z","steps":["trace[605590752] 'process raft request'  (duration: 83.039ms)","trace[605590752] 'compare'  (duration: 40.9342ms)"],"step_count":2}
{"level":"info","ts":"2025-05-01T12:11:52.380437Z","caller":"traceutil/trace.go:171","msg":"trace[574595508] transaction","detail":"{read_only:false; response_revision:3291; number_of_response:1; }","duration":"114.5968ms","start":"2025-05-01T12:11:52.265773Z","end":"2025-05-01T12:11:52.380370Z","steps":["trace[574595508] 'process raft request'  (duration: 54.1378ms)","trace[574595508] 'compare'  (duration: 59.8621ms)"],"step_count":2}
{"level":"info","ts":"2025-05-01T12:12:00.665975Z","caller":"traceutil/trace.go:171","msg":"trace[1830007897] linearizableReadLoop","detail":"{readStateIndex:3995; appliedIndex:3994; }","duration":"104.0329ms","start":"2025-05-01T12:12:00.561912Z","end":"2025-05-01T12:12:00.665945Z","steps":["trace[1830007897] 'read index received'  (duration: 103.8232ms)","trace[1830007897] 'applied index is now lower than readState.Index'  (duration: 208.7µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-01T12:12:00.666155Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.2178ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:12:00.666214Z","caller":"traceutil/trace.go:171","msg":"trace[2026190770] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3297; }","duration":"104.3246ms","start":"2025-05-01T12:12:00.561871Z","end":"2025-05-01T12:12:00.666196Z","steps":["trace[2026190770] 'agreement among raft nodes before linearized reading'  (duration: 104.1838ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:12:00.666654Z","caller":"traceutil/trace.go:171","msg":"trace[1195051772] transaction","detail":"{read_only:false; response_revision:3297; number_of_response:1; }","duration":"161.8955ms","start":"2025-05-01T12:12:00.504732Z","end":"2025-05-01T12:12:00.666627Z","steps":["trace[1195051772] 'process raft request'  (duration: 161.0741ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:12:41.993864Z","caller":"traceutil/trace.go:171","msg":"trace[1221120513] linearizableReadLoop","detail":"{readStateIndex:4035; appliedIndex:4034; }","duration":"274.7128ms","start":"2025-05-01T12:12:41.719118Z","end":"2025-05-01T12:12:41.993831Z","steps":["trace[1221120513] 'read index received'  (duration: 274.4293ms)","trace[1221120513] 'applied index is now lower than readState.Index'  (duration: 282.2µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-01T12:12:41.994373Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"275.2709ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:12:41.994455Z","caller":"traceutil/trace.go:171","msg":"trace[102499846] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3329; }","duration":"275.4162ms","start":"2025-05-01T12:12:41.719016Z","end":"2025-05-01T12:12:41.994432Z","steps":["trace[102499846] 'agreement among raft nodes before linearized reading'  (duration: 275.0918ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:12:41.994973Z","caller":"traceutil/trace.go:171","msg":"trace[1330643693] transaction","detail":"{read_only:false; response_revision:3329; number_of_response:1; }","duration":"288.2087ms","start":"2025-05-01T12:12:41.706733Z","end":"2025-05-01T12:12:41.994942Z","steps":["trace[1330643693] 'process raft request'  (duration: 286.893ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:12:47.432284Z","caller":"traceutil/trace.go:171","msg":"trace[1567251325] transaction","detail":"{read_only:false; response_revision:3334; number_of_response:1; }","duration":"197.5535ms","start":"2025-05-01T12:12:47.234692Z","end":"2025-05-01T12:12:47.432246Z","steps":["trace[1567251325] 'process raft request'  (duration: 197.0956ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:13:04.946476Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.8257ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingadmissionpolicybindings/\" range_end:\"/registry/validatingadmissionpolicybindings0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:13:04.946644Z","caller":"traceutil/trace.go:171","msg":"trace[1357180305] range","detail":"{range_begin:/registry/validatingadmissionpolicybindings/; range_end:/registry/validatingadmissionpolicybindings0; response_count:0; response_revision:3347; }","duration":"105.0896ms","start":"2025-05-01T12:13:04.841529Z","end":"2025-05-01T12:13:04.946618Z","steps":["trace[1357180305] 'count revisions from in-memory index tree'  (duration: 104.6729ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:13:06.883620Z","caller":"traceutil/trace.go:171","msg":"trace[1795092429] transaction","detail":"{read_only:false; response_revision:3348; number_of_response:1; }","duration":"110.2767ms","start":"2025-05-01T12:13:06.773315Z","end":"2025-05-01T12:13:06.883592Z","steps":["trace[1795092429] 'process raft request'  (duration: 110.0662ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:13:07.955416Z","caller":"traceutil/trace.go:171","msg":"trace[1148001470] transaction","detail":"{read_only:false; response_revision:3349; number_of_response:1; }","duration":"180.6832ms","start":"2025-05-01T12:13:07.774704Z","end":"2025-05-01T12:13:07.955387Z","steps":["trace[1148001470] 'process raft request'  (duration: 180.055ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:13:08.105273Z","caller":"traceutil/trace.go:171","msg":"trace[129829681] transaction","detail":"{read_only:false; response_revision:3350; number_of_response:1; }","duration":"227.1126ms","start":"2025-05-01T12:13:07.878129Z","end":"2025-05-01T12:13:08.105242Z","steps":["trace[129829681] 'process raft request'  (duration: 141.6761ms)","trace[129829681] 'compare'  (duration: 85.2152ms)"],"step_count":2}
{"level":"info","ts":"2025-05-01T12:13:11.430027Z","caller":"traceutil/trace.go:171","msg":"trace[2007711226] transaction","detail":"{read_only:false; response_revision:3352; number_of_response:1; }","duration":"465.357ms","start":"2025-05-01T12:13:10.964645Z","end":"2025-05-01T12:13:11.430002Z","steps":["trace[2007711226] 'process raft request'  (duration: 465.235ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:13:11.430200Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-01T12:13:10.964619Z","time spent":"465.4901ms","remote":"127.0.0.1:49956","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:3351 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-05-01T12:13:36.430872Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-05-01T12:13:36.430945Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-05-01T12:13:36.431105Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-01T12:13:36.437131Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
2025/05/01 12:13:36 WARNING: [core] [Server #8] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2025-05-01T12:13:36.442292Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.5154ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 ","response":"","error":"context canceled"}
{"level":"info","ts":"2025-05-01T12:13:36.442392Z","caller":"traceutil/trace.go:171","msg":"trace[1014296998] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; }","duration":"105.6736ms","start":"2025-05-01T12:13:36.336694Z","end":"2025-05-01T12:13:36.442367Z","steps":["trace[1014296998] 'agreement among raft nodes before linearized reading'  (duration: 105.5506ms)"],"step_count":1}
2025/05/01 12:13:36 WARNING: [core] [Server #8] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"info","ts":"2025-05-01T12:13:36.470241Z","caller":"traceutil/trace.go:171","msg":"trace[1063490174] linearizableReadLoop","detail":"{readStateIndex:4092; appliedIndex:4091; }","duration":"133.4735ms","start":"2025-05-01T12:13:36.336739Z","end":"2025-05-01T12:13:36.470213Z","steps":["trace[1063490174] 'read index received'  (duration: 37.8702ms)","trace[1063490174] 'applied index is now lower than readState.Index'  (duration: 95.602ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-01T12:13:36.769686Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-01T12:13:36.769779Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-05-01T12:13:36.769924Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-05-01T12:13:36.949425Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-01T12:13:36.950012Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-01T12:13:36.950044Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [e7daf90fb83f] <==
{"level":"info","ts":"2025-05-01T12:30:13.531333Z","caller":"traceutil/trace.go:171","msg":"trace[1182395779] linearizableReadLoop","detail":"{readStateIndex:5122; appliedIndex:5121; }","duration":"106.3286ms","start":"2025-05-01T12:30:13.424966Z","end":"2025-05-01T12:30:13.531295Z","steps":["trace[1182395779] 'read index received'  (duration: 58.2µs)","trace[1182395779] 'applied index is now lower than readState.Index'  (duration: 106.2679ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-01T12:30:13.531444Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.5237ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:30:13.531556Z","caller":"traceutil/trace.go:171","msg":"trace[2129375066] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4211; }","duration":"106.6781ms","start":"2025-05-01T12:30:13.424860Z","end":"2025-05-01T12:30:13.531538Z","steps":["trace[2129375066] 'agreement among raft nodes before linearized reading'  (duration: 106.5268ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:30:16.629932Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"146.436ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:30:16.630023Z","caller":"traceutil/trace.go:171","msg":"trace[619642151] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4214; }","duration":"146.598ms","start":"2025-05-01T12:30:16.483401Z","end":"2025-05-01T12:30:16.629999Z","steps":["trace[619642151] 'range keys from in-memory index tree'  (duration: 146.3217ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:30:20.689321Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3979}
{"level":"info","ts":"2025-05-01T12:30:20.724382Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3979,"took":"34.6421ms","hash":113784426,"current-db-size-bytes":3260416,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1757184,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-05-01T12:30:20.724518Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":113784426,"revision":3979,"compact-revision":3736}
{"level":"warn","ts":"2025-05-01T12:30:33.427109Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.6799ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036954559071078 > lease_revoke:<id:70cc968bc5bdc30b>","response":"size:29"}
{"level":"info","ts":"2025-05-01T12:30:37.013664Z","caller":"traceutil/trace.go:171","msg":"trace[926166223] linearizableReadLoop","detail":"{readStateIndex:5147; appliedIndex:5146; }","duration":"124.3909ms","start":"2025-05-01T12:30:36.885784Z","end":"2025-05-01T12:30:37.010175Z","steps":["trace[926166223] 'read index received'  (duration: 124.159ms)","trace[926166223] 'applied index is now lower than readState.Index'  (duration: 228.3µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-01T12:30:37.013859Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"128.0557ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:30:37.013913Z","caller":"traceutil/trace.go:171","msg":"trace[1927921180] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4231; }","duration":"128.1229ms","start":"2025-05-01T12:30:36.885773Z","end":"2025-05-01T12:30:37.013896Z","steps":["trace[1927921180] 'agreement among raft nodes before linearized reading'  (duration: 127.9854ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:30:37.014370Z","caller":"traceutil/trace.go:171","msg":"trace[1077102470] transaction","detail":"{read_only:false; response_revision:4231; number_of_response:1; }","duration":"226.387ms","start":"2025-05-01T12:30:36.787959Z","end":"2025-05-01T12:30:37.014346Z","steps":["trace[1077102470] 'process raft request'  (duration: 222.0643ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:30:46.896053Z","caller":"traceutil/trace.go:171","msg":"trace[670847110] transaction","detail":"{read_only:false; response_revision:4238; number_of_response:1; }","duration":"107.1527ms","start":"2025-05-01T12:30:46.788870Z","end":"2025-05-01T12:30:46.896023Z","steps":["trace[670847110] 'process raft request'  (duration: 106.6399ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:31:13.581684Z","caller":"traceutil/trace.go:171","msg":"trace[442475946] linearizableReadLoop","detail":"{readStateIndex:5186; appliedIndex:5185; }","duration":"151.7645ms","start":"2025-05-01T12:31:13.429876Z","end":"2025-05-01T12:31:13.581640Z","steps":["trace[442475946] 'read index received'  (duration: 120.6678ms)","trace[442475946] 'applied index is now lower than readState.Index'  (duration: 31.0949ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-01T12:31:13.581978Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"152.1445ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:31:13.582040Z","caller":"traceutil/trace.go:171","msg":"trace[1240905319] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4261; }","duration":"152.2562ms","start":"2025-05-01T12:31:13.429763Z","end":"2025-05-01T12:31:13.582019Z","steps":["trace[1240905319] 'agreement among raft nodes before linearized reading'  (duration: 152.1025ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:32:01.568856Z","caller":"traceutil/trace.go:171","msg":"trace[1230149382] linearizableReadLoop","detail":"{readStateIndex:5234; appliedIndex:5233; }","duration":"144.1394ms","start":"2025-05-01T12:32:01.424685Z","end":"2025-05-01T12:32:01.568824Z","steps":["trace[1230149382] 'read index received'  (duration: 143.8076ms)","trace[1230149382] 'applied index is now lower than readState.Index'  (duration: 330.2µs)"],"step_count":2}
{"level":"info","ts":"2025-05-01T12:32:01.569011Z","caller":"traceutil/trace.go:171","msg":"trace[598341362] transaction","detail":"{read_only:false; response_revision:4300; number_of_response:1; }","duration":"190.2969ms","start":"2025-05-01T12:32:01.378687Z","end":"2025-05-01T12:32:01.568984Z","steps":["trace[598341362] 'process raft request'  (duration: 189.8898ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:32:01.569062Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"144.3565ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:32:01.569126Z","caller":"traceutil/trace.go:171","msg":"trace[203488302] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4300; }","duration":"144.4679ms","start":"2025-05-01T12:32:01.424638Z","end":"2025-05-01T12:32:01.569106Z","steps":["trace[203488302] 'agreement among raft nodes before linearized reading'  (duration: 144.3605ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:32:54.092359Z","caller":"traceutil/trace.go:171","msg":"trace[2122794081] transaction","detail":"{read_only:false; response_revision:4344; number_of_response:1; }","duration":"111.4953ms","start":"2025-05-01T12:32:53.980828Z","end":"2025-05-01T12:32:54.092324Z","steps":["trace[2122794081] 'process raft request'  (duration: 111.2612ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:33:20.841174Z","caller":"traceutil/trace.go:171","msg":"trace[1094543699] transaction","detail":"{read_only:false; response_revision:4365; number_of_response:1; }","duration":"181.8261ms","start":"2025-05-01T12:33:20.659316Z","end":"2025-05-01T12:33:20.841142Z","steps":["trace[1094543699] 'process raft request'  (duration: 181.5849ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:34:04.324180Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.6795ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/poddisruptionbudgets/\" range_end:\"/registry/poddisruptionbudgets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:34:04.324276Z","caller":"traceutil/trace.go:171","msg":"trace[598687462] range","detail":"{range_begin:/registry/poddisruptionbudgets/; range_end:/registry/poddisruptionbudgets0; response_count:0; response_revision:4398; }","duration":"136.8273ms","start":"2025-05-01T12:34:04.187427Z","end":"2025-05-01T12:34:04.324255Z","steps":["trace[598687462] 'count revisions from in-memory index tree'  (duration: 136.5756ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:34:08.106382Z","caller":"traceutil/trace.go:171","msg":"trace[1039861678] transaction","detail":"{read_only:false; response_revision:4401; number_of_response:1; }","duration":"120.5887ms","start":"2025-05-01T12:34:07.985760Z","end":"2025-05-01T12:34:08.106349Z","steps":["trace[1039861678] 'process raft request'  (duration: 46.9947ms)","trace[1039861678] 'compare'  (duration: 73.3307ms)"],"step_count":2}
{"level":"info","ts":"2025-05-01T12:34:08.108741Z","caller":"traceutil/trace.go:171","msg":"trace[686895538] transaction","detail":"{read_only:false; response_revision:4402; number_of_response:1; }","duration":"116.1434ms","start":"2025-05-01T12:34:07.992562Z","end":"2025-05-01T12:34:08.108705Z","steps":["trace[686895538] 'process raft request'  (duration: 115.9282ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:35:20.769641Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4219}
{"level":"info","ts":"2025-05-01T12:35:20.807268Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4219,"took":"37.2345ms","hash":3221584905,"current-db-size-bytes":3260416,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1748992,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-05-01T12:35:20.807391Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3221584905,"revision":4219,"compact-revision":3979}
{"level":"info","ts":"2025-05-01T12:36:02.722728Z","caller":"traceutil/trace.go:171","msg":"trace[417457829] transaction","detail":"{read_only:false; response_revision:4494; number_of_response:1; }","duration":"143.7361ms","start":"2025-05-01T12:36:02.578957Z","end":"2025-05-01T12:36:02.722693Z","steps":["trace[417457829] 'process raft request'  (duration: 142.8611ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:36:17.120205Z","caller":"traceutil/trace.go:171","msg":"trace[1333810011] transaction","detail":"{read_only:false; response_revision:4505; number_of_response:1; }","duration":"105.368ms","start":"2025-05-01T12:36:17.014812Z","end":"2025-05-01T12:36:17.120180Z","steps":["trace[1333810011] 'process raft request'  (duration: 105.1896ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:37:19.471415Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"578.2945ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:37:19.471747Z","caller":"traceutil/trace.go:171","msg":"trace[1069985316] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4554; }","duration":"578.6341ms","start":"2025-05-01T12:37:18.893086Z","end":"2025-05-01T12:37:19.471720Z","steps":["trace[1069985316] 'range keys from in-memory index tree'  (duration: 578.2739ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:38:04.042600Z","caller":"traceutil/trace.go:171","msg":"trace[2095772405] transaction","detail":"{read_only:false; response_revision:4592; number_of_response:1; }","duration":"134.4849ms","start":"2025-05-01T12:38:03.908079Z","end":"2025-05-01T12:38:04.042563Z","steps":["trace[2095772405] 'process raft request'  (duration: 134.071ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-01T12:39:53.525577Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"185.867ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036954559073771 > lease_revoke:<id:70cc968bc5bdcd9f>","response":"size:29"}
{"level":"info","ts":"2025-05-01T12:40:20.833358Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4459}
{"level":"info","ts":"2025-05-01T12:40:20.871320Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4459,"took":"37.502ms","hash":2680384766,"current-db-size-bytes":3260416,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1769472,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-05-01T12:40:20.871444Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2680384766,"revision":4459,"compact-revision":4219}
{"level":"info","ts":"2025-05-01T12:41:02.771697Z","caller":"traceutil/trace.go:171","msg":"trace[1548019646] transaction","detail":"{read_only:false; response_revision:4735; number_of_response:1; }","duration":"168.0008ms","start":"2025-05-01T12:41:02.603668Z","end":"2025-05-01T12:41:02.771669Z","steps":["trace[1548019646] 'process raft request'  (duration: 167.7248ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:41:11.009847Z","caller":"traceutil/trace.go:171","msg":"trace[2042572278] transaction","detail":"{read_only:false; response_revision:4742; number_of_response:1; }","duration":"100.9532ms","start":"2025-05-01T12:41:10.908865Z","end":"2025-05-01T12:41:11.009818Z","steps":["trace[2042572278] 'process raft request'  (duration: 87.0447ms)","trace[2042572278] 'compare'  (duration: 13.728ms)"],"step_count":2}
{"level":"info","ts":"2025-05-01T12:41:27.688255Z","caller":"traceutil/trace.go:171","msg":"trace[1887858049] linearizableReadLoop","detail":"{readStateIndex:5805; appliedIndex:5804; }","duration":"241.2036ms","start":"2025-05-01T12:41:27.447022Z","end":"2025-05-01T12:41:27.688225Z","steps":["trace[1887858049] 'read index received'  (duration: 241.1349ms)","trace[1887858049] 'applied index is now lower than readState.Index'  (duration: 67.2µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-01T12:41:27.688505Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"241.5197ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:41:27.688569Z","caller":"traceutil/trace.go:171","msg":"trace[2078826631] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4754; }","duration":"241.6363ms","start":"2025-05-01T12:41:27.446917Z","end":"2025-05-01T12:41:27.688553Z","steps":["trace[2078826631] 'agreement among raft nodes before linearized reading'  (duration: 241.4244ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:41:27.688570Z","caller":"traceutil/trace.go:171","msg":"trace[1002158457] transaction","detail":"{read_only:false; response_revision:4754; number_of_response:1; }","duration":"253.538ms","start":"2025-05-01T12:41:27.434831Z","end":"2025-05-01T12:41:27.688369Z","steps":["trace[1002158457] 'process raft request'  (duration: 253.2229ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:42:04.575943Z","caller":"traceutil/trace.go:171","msg":"trace[1091031463] linearizableReadLoop","detail":"{readStateIndex:5842; appliedIndex:5841; }","duration":"109.4723ms","start":"2025-05-01T12:42:04.466439Z","end":"2025-05-01T12:42:04.575911Z","steps":["trace[1091031463] 'read index received'  (duration: 109.0299ms)","trace[1091031463] 'applied index is now lower than readState.Index'  (duration: 441.4µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-01T12:42:04.576198Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.7288ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:42:04.576322Z","caller":"traceutil/trace.go:171","msg":"trace[872880916] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4783; }","duration":"109.8503ms","start":"2025-05-01T12:42:04.466395Z","end":"2025-05-01T12:42:04.576245Z","steps":["trace[872880916] 'agreement among raft nodes before linearized reading'  (duration: 109.7058ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:42:04.576711Z","caller":"traceutil/trace.go:171","msg":"trace[1141623514] transaction","detail":"{read_only:false; response_revision:4783; number_of_response:1; }","duration":"169.7396ms","start":"2025-05-01T12:42:04.406947Z","end":"2025-05-01T12:42:04.576686Z","steps":["trace[1141623514] 'process raft request'  (duration: 168.8047ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:44:24.088448Z","caller":"traceutil/trace.go:171","msg":"trace[1864506930] transaction","detail":"{read_only:false; response_revision:4895; number_of_response:1; }","duration":"104.8478ms","start":"2025-05-01T12:44:23.983573Z","end":"2025-05-01T12:44:24.088421Z","steps":["trace[1864506930] 'process raft request'  (duration: 104.5905ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:45:20.590785Z","caller":"traceutil/trace.go:171","msg":"trace[756827669] transaction","detail":"{read_only:false; response_revision:4940; number_of_response:1; }","duration":"171.5906ms","start":"2025-05-01T12:45:20.419169Z","end":"2025-05-01T12:45:20.590759Z","steps":["trace[756827669] 'process raft request'  (duration: 171.3204ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:45:20.994422Z","caller":"traceutil/trace.go:171","msg":"trace[1499221719] linearizableReadLoop","detail":"{readStateIndex:6039; appliedIndex:6038; }","duration":"101.9159ms","start":"2025-05-01T12:45:20.892475Z","end":"2025-05-01T12:45:20.994391Z","steps":["trace[1499221719] 'read index received'  (duration: 101.2113ms)","trace[1499221719] 'applied index is now lower than readState.Index'  (duration: 702.7µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-01T12:45:20.994594Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.1022ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-01T12:45:20.994656Z","caller":"traceutil/trace.go:171","msg":"trace[1492912687] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4941; }","duration":"102.1689ms","start":"2025-05-01T12:45:20.892464Z","end":"2025-05-01T12:45:20.994633Z","steps":["trace[1492912687] 'agreement among raft nodes before linearized reading'  (duration: 102.0476ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:45:20.994997Z","caller":"traceutil/trace.go:171","msg":"trace[978287413] transaction","detail":"{read_only:false; response_revision:4941; number_of_response:1; }","duration":"155.7178ms","start":"2025-05-01T12:45:20.839259Z","end":"2025-05-01T12:45:20.994977Z","steps":["trace[978287413] 'process raft request'  (duration: 154.8579ms)"],"step_count":1}
{"level":"info","ts":"2025-05-01T12:45:21.150611Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4700}
{"level":"warn","ts":"2025-05-01T12:45:21.165318Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.6985ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036954559075353 username:\"kube-apiserver-etcd-client\" auth_revision:1 > compaction:<revision:4700 > ","response":"size:5"}
{"level":"info","ts":"2025-05-01T12:45:21.228946Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4700,"took":"77.9123ms","hash":2825553676,"current-db-size-bytes":3260416,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1761280,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-05-01T12:45:21.229211Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2825553676,"revision":4700,"compact-revision":4459}
{"level":"info","ts":"2025-05-01T12:45:21.229293Z","caller":"traceutil/trace.go:171","msg":"trace[1318407049] compact","detail":"{revision:4700; response_revision:4941; }","duration":"232.6475ms","start":"2025-05-01T12:45:20.996624Z","end":"2025-05-01T12:45:21.229271Z","steps":["trace[1318407049] 'process raft request'  (duration: 29.1573ms)","trace[1318407049] 'check and update compact revision'  (duration: 124.5989ms)"],"step_count":2}


==> kernel <==
 12:45:52 up  7:21,  0 users,  load average: 0.98, 0.75, 1.08
Linux minikube 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [6dbc24eecdd4] <==
W0501 12:15:24.144361       1 genericapiserver.go:767] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0501 12:15:24.192109       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0501 12:15:24.192254       1 genericapiserver.go:767] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0501 12:15:27.808509       1 secure_serving.go:213] Serving securely on [::]:8443
I0501 12:15:27.808816       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0501 12:15:27.809081       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0501 12:15:27.818518       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0501 12:15:27.821154       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0501 12:15:27.821238       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0501 12:15:27.821724       1 aggregator.go:169] waiting for initial CRD sync...
I0501 12:15:27.822139       1 local_available_controller.go:156] Starting LocalAvailability controller
I0501 12:15:27.822578       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0501 12:15:27.822697       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0501 12:15:27.823201       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0501 12:15:27.823529       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0501 12:15:27.823613       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0501 12:15:27.852263       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0501 12:15:27.852724       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0501 12:15:27.853009       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0501 12:15:27.853950       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0501 12:15:27.854782       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0501 12:15:27.853145       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0501 12:15:27.854810       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0501 12:15:27.853457       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0501 12:15:27.855897       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0501 12:15:27.853504       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0501 12:15:27.855934       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0501 12:15:27.853530       1 controller.go:78] Starting OpenAPI AggregationController
I0501 12:15:27.853877       1 controller.go:119] Starting legacy_token_tracking_controller
I0501 12:15:27.856470       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0501 12:15:27.854550       1 controller.go:142] Starting OpenAPI controller
I0501 12:15:27.854646       1 controller.go:90] Starting OpenAPI V3 controller
I0501 12:15:27.854674       1 naming_controller.go:294] Starting NamingConditionController
I0501 12:15:27.854692       1 establishing_controller.go:81] Starting EstablishingController
I0501 12:15:27.854709       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0501 12:15:27.854723       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0501 12:15:27.854738       1 crd_finalizer.go:269] Starting CRDFinalizer
I0501 12:15:28.250672       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0501 12:15:28.252946       1 aggregator.go:171] initial CRD sync complete...
I0501 12:15:28.253009       1 autoregister_controller.go:144] Starting autoregister controller
I0501 12:15:28.253027       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0501 12:15:28.561917       1 shared_informer.go:320] Caches are synced for node_authorizer
I0501 12:15:28.652775       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0501 12:15:28.652826       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0501 12:15:28.736816       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0501 12:15:28.736940       1 shared_informer.go:320] Caches are synced for configmaps
I0501 12:15:28.740279       1 cache.go:39] Caches are synced for LocalAvailability controller
I0501 12:15:28.747491       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0501 12:15:28.755978       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0501 12:15:28.756565       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0501 12:15:28.756595       1 policy_source.go:240] refreshing policies
I0501 12:15:28.758699       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0501 12:15:28.836556       1 cache.go:39] Caches are synced for autoregister controller
I0501 12:15:28.851529       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0501 12:15:28.939918       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0501 12:15:29.130057       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0501 12:15:44.152230       1 controller.go:615] quota admission added evaluator for: endpoints
I0501 12:15:44.167158       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0501 12:15:44.238855       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0501 12:15:44.454263       1 controller.go:615] quota admission added evaluator for: deployments.apps


==> kube-apiserver [9da2cbb5a94d] <==
W0501 12:13:36.747570       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:36.747683       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I0501 12:13:36.747763       1 dynamic_cafile_content.go:175] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0501 12:13:36.747794       1 dynamic_cafile_content.go:175] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0501 12:13:36.751005       1 dynamic_cafile_content.go:175] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
W0501 12:13:36.752874       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.434988       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.448543       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.448811       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.467069       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.467314       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.467603       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.467779       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.467922       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.468040       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.468167       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.468280       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.468418       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.468514       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.468605       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.468742       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.468863       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.469066       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.550964       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.635695       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.652379       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.652597       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.652931       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.653081       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.653224       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.653285       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.653342       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.653517       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.653627       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.653867       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.654120       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.665509       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.665703       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.666170       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.667467       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.667632       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.667765       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.667889       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.668009       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.732128       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.732353       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.732602       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.732806       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.732956       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.733093       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.733148       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.733221       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.733393       1 logging.go:55] [core] [Channel #18 SubChannel #19]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.734046       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.734054       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.736672       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.737469       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.751379       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.751647       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0501 12:13:37.753538       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [7489582f365c] <==
I0501 12:11:25.629272       1 attach_detach_controller.go:338] "Starting attach detach controller" logger="persistentvolume-attach-detach-controller"
I0501 12:11:25.629398       1 shared_informer.go:313] Waiting for caches to sync for attach detach
I0501 12:11:25.923164       1 controllermanager.go:765] "Started controller" controller="validatingadmissionpolicy-status-controller"
I0501 12:11:25.923265       1 shared_informer.go:313] Waiting for caches to sync for validatingadmissionpolicy-status
I0501 12:11:25.973932       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0501 12:11:26.165918       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0501 12:11:26.257462       1 shared_informer.go:320] Caches are synced for cronjob
I0501 12:11:26.278580       1 shared_informer.go:320] Caches are synced for TTL after finished
I0501 12:11:26.356020       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0501 12:11:26.356100       1 shared_informer.go:320] Caches are synced for PV protection
I0501 12:11:26.359105       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0501 12:11:26.363238       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0501 12:11:26.363415       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0501 12:11:26.370296       1 shared_informer.go:320] Caches are synced for namespace
I0501 12:11:26.373009       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0501 12:11:26.390989       1 shared_informer.go:320] Caches are synced for service account
I0501 12:11:26.397952       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0501 12:11:26.398289       1 shared_informer.go:320] Caches are synced for job
I0501 12:11:26.398764       1 shared_informer.go:320] Caches are synced for crt configmap
I0501 12:11:26.402351       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0501 12:11:26.402743       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="208µs"
I0501 12:11:26.428334       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0501 12:11:26.436553       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0501 12:11:26.437573       1 shared_informer.go:320] Caches are synced for expand
I0501 12:11:26.437778       1 shared_informer.go:320] Caches are synced for disruption
I0501 12:11:26.438022       1 shared_informer.go:320] Caches are synced for stateful set
I0501 12:11:26.438208       1 shared_informer.go:320] Caches are synced for endpoint
I0501 12:11:26.441344       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="60.0502ms"
I0501 12:11:26.441490       1 shared_informer.go:320] Caches are synced for PVC protection
I0501 12:11:26.452305       1 shared_informer.go:320] Caches are synced for ephemeral
I0501 12:11:26.452597       1 shared_informer.go:320] Caches are synced for HPA
I0501 12:11:26.528572       1 shared_informer.go:320] Caches are synced for ReplicationController
I0501 12:11:26.529382       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0501 12:11:26.529491       1 shared_informer.go:320] Caches are synced for TTL
I0501 12:11:26.539946       1 shared_informer.go:320] Caches are synced for deployment
I0501 12:11:26.564993       1 shared_informer.go:320] Caches are synced for taint
I0501 12:11:26.565517       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0501 12:11:26.565863       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0501 12:11:26.566027       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0501 12:11:26.566109       1 shared_informer.go:320] Caches are synced for persistent volume
I0501 12:11:26.566586       1 shared_informer.go:320] Caches are synced for GC
I0501 12:11:26.566864       1 shared_informer.go:320] Caches are synced for daemon sets
I0501 12:11:26.566987       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0501 12:11:26.631459       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0501 12:11:26.649118       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0501 12:11:26.659923       1 shared_informer.go:320] Caches are synced for node
I0501 12:11:26.660093       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0501 12:11:26.660176       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0501 12:11:26.660197       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0501 12:11:26.660213       1 shared_informer.go:320] Caches are synced for cidrallocator
I0501 12:11:26.660446       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0501 12:11:26.660512       1 shared_informer.go:320] Caches are synced for resource quota
I0501 12:11:26.660561       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0501 12:11:26.660644       1 shared_informer.go:320] Caches are synced for attach detach
I0501 12:11:26.731756       1 shared_informer.go:320] Caches are synced for resource quota
I0501 12:11:26.760472       1 shared_informer.go:320] Caches are synced for garbage collector
I0501 12:11:26.828233       1 shared_informer.go:320] Caches are synced for garbage collector
I0501 12:11:26.828278       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0501 12:11:26.828296       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0501 12:11:44.669825       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [a09582a8f6b7] <==
I0501 12:15:43.837706       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0501 12:15:43.838254       1 shared_informer.go:320] Caches are synced for attach detach
I0501 12:15:43.838915       1 shared_informer.go:320] Caches are synced for stateful set
I0501 12:15:43.838918       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0501 12:15:43.838942       1 shared_informer.go:320] Caches are synced for PVC protection
I0501 12:15:43.841467       1 shared_informer.go:320] Caches are synced for resource quota
I0501 12:15:43.841775       1 shared_informer.go:320] Caches are synced for TTL
I0501 12:15:43.848346       1 shared_informer.go:320] Caches are synced for GC
I0501 12:15:43.865371       1 shared_informer.go:320] Caches are synced for crt configmap
I0501 12:15:43.865749       1 shared_informer.go:320] Caches are synced for garbage collector
I0501 12:15:43.865870       1 shared_informer.go:320] Caches are synced for cronjob
I0501 12:15:43.865749       1 shared_informer.go:320] Caches are synced for daemon sets
I0501 12:15:43.865873       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0501 12:15:43.866331       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0501 12:15:43.878240       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0501 12:15:43.865759       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0501 12:15:43.865771       1 shared_informer.go:320] Caches are synced for TTL after finished
I0501 12:15:43.878976       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0501 12:15:43.865794       1 shared_informer.go:320] Caches are synced for disruption
I0501 12:15:43.865819       1 shared_informer.go:320] Caches are synced for node
I0501 12:15:43.883196       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0501 12:15:43.883239       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0501 12:15:43.883256       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0501 12:15:43.883271       1 shared_informer.go:320] Caches are synced for cidrallocator
I0501 12:15:43.883365       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0501 12:15:43.878407       1 shared_informer.go:320] Caches are synced for endpoint
I0501 12:15:43.884924       1 shared_informer.go:320] Caches are synced for ephemeral
I0501 12:15:43.933111       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0501 12:15:43.934776       1 shared_informer.go:320] Caches are synced for ReplicationController
I0501 12:15:43.947963       1 shared_informer.go:320] Caches are synced for garbage collector
I0501 12:15:44.338355       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="505.5086ms"
I0501 12:15:44.338539       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="105.4µs"
I0501 12:15:44.565526       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="102.1µs"
I0501 12:15:45.486325       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="71.5µs"
I0501 12:15:56.623911       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="96.02ms"
I0501 12:15:56.625291       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="89.5µs"
I0501 12:15:57.326025       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="81.9µs"
I0501 12:16:14.405099       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="77.7µs"
I0501 12:16:26.406537       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="71.2µs"
I0501 12:16:45.357930       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="84.4µs"
I0501 12:16:59.310855       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="78.8µs"
I0501 12:17:28.342761       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="91µs"
I0501 12:17:40.383432       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="88.9µs"
I0501 12:19:03.313149       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="82.1µs"
I0501 12:19:16.288902       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="65.2µs"
I0501 12:21:48.238982       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="105.4µs"
I0501 12:22:00.256203       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="112.3µs"
I0501 12:22:23.335134       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0501 12:27:07.256363       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="183.7µs"
I0501 12:27:18.245335       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="142µs"
I0501 12:27:29.511831       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0501 12:32:12.258498       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="192µs"
I0501 12:32:23.324312       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="171.9µs"
I0501 12:32:36.417357       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0501 12:37:20.235718       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="149.9µs"
I0501 12:37:33.294759       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="85.9µs"
I0501 12:37:43.454389       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0501 12:42:33.334643       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="201.5µs"
I0501 12:42:45.288382       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-6f8c5d596d" duration="78.1µs"
I0501 12:42:50.636372       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [0be714d64709] <==
I0501 12:10:49.547200       1 server_linux.go:66] "Using iptables proxy"
E0501 12:11:01.557406       1 server.go:687] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": net/http: TLS handshake timeout"
E0501 12:11:12.693210       1 server.go:687] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": net/http: TLS handshake timeout"
I0501 12:11:15.470909       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0501 12:11:15.472196       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0501 12:11:16.452719       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0501 12:11:16.452834       1 server_linux.go:170] "Using iptables Proxier"
I0501 12:11:16.494333       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0501 12:11:16.498024       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0501 12:11:16.556639       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0501 12:11:16.573541       1 server.go:497] "Version info" version="v1.32.0"
I0501 12:11:16.573613       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0501 12:11:16.657895       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0501 12:11:16.668209       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0501 12:11:16.696723       1 config.go:199] "Starting service config controller"
I0501 12:11:16.697517       1 shared_informer.go:313] Waiting for caches to sync for service config
I0501 12:11:16.697720       1 config.go:105] "Starting endpoint slice config controller"
I0501 12:11:16.749767       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0501 12:11:16.709397       1 config.go:329] "Starting node config controller"
I0501 12:11:16.749874       1 shared_informer.go:313] Waiting for caches to sync for node config
I0501 12:11:16.798464       1 shared_informer.go:320] Caches are synced for service config
I0501 12:11:16.851378       1 shared_informer.go:320] Caches are synced for node config
I0501 12:11:16.950513       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [42c527f93188] <==
I0501 12:15:39.888212       1 server_linux.go:66] "Using iptables proxy"
I0501 12:15:40.546618       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0501 12:15:40.546824       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0501 12:15:40.740121       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0501 12:15:40.740806       1 server_linux.go:170] "Using iptables Proxier"
I0501 12:15:40.768956       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0501 12:15:40.771599       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0501 12:15:40.830951       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0501 12:15:40.831316       1 server.go:497] "Version info" version="v1.32.0"
I0501 12:15:40.831394       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0501 12:15:40.833804       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0501 12:15:40.837577       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0501 12:15:40.847483       1 config.go:199] "Starting service config controller"
I0501 12:15:40.847742       1 shared_informer.go:313] Waiting for caches to sync for service config
I0501 12:15:40.847909       1 config.go:105] "Starting endpoint slice config controller"
I0501 12:15:40.847925       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0501 12:15:40.849996       1 config.go:329] "Starting node config controller"
I0501 12:15:40.850061       1 shared_informer.go:313] Waiting for caches to sync for node config
I0501 12:15:40.964424       1 shared_informer.go:320] Caches are synced for service config
I0501 12:15:40.964500       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0501 12:15:40.957376       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [6f94f4546409] <==
I0501 12:11:00.451379       1 serving.go:386] Generated self-signed cert in-memory
I0501 12:11:15.465305       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0501 12:11:15.465421       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0501 12:11:15.762387       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0501 12:11:15.773015       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0501 12:11:15.773384       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0501 12:11:15.776049       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0501 12:11:15.778887       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0501 12:11:15.778945       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0501 12:11:15.778992       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0501 12:11:15.779015       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0501 12:11:16.053093       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0501 12:11:16.053170       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0501 12:11:16.200701       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I0501 12:13:36.631176       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0501 12:13:36.631285       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0501 12:13:36.631894       1 requestheader_controller.go:194] Shutting down RequestHeaderAuthRequestController
I0501 12:13:36.631942       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0501 12:13:36.631998       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0501 12:13:36.632843       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [d1ae22961895] <==
I0501 12:15:17.951591       1 serving.go:386] Generated self-signed cert in-memory
W0501 12:15:28.335676       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0501 12:15:28.341126       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, role.rbac.authorization.k8s.io "extension-apiserver-authentication-reader" not found, role.rbac.authorization.k8s.io "system::leader-locking-kube-scheduler" not found]
W0501 12:15:28.341289       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0501 12:15:28.341311       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0501 12:15:28.572041       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0501 12:15:28.572989       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0501 12:15:28.586867       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0501 12:15:28.591765       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0501 12:15:28.591819       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0501 12:15:28.591871       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0501 12:15:28.950224       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 01 12:34:45 minikube kubelet[1614]: E0501 12:34:45.174667    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:34:58 minikube kubelet[1614]: E0501 12:34:58.178118    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:35:09 minikube kubelet[1614]: E0501 12:35:09.178498    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:35:22 minikube kubelet[1614]: E0501 12:35:22.178206    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:35:37 minikube kubelet[1614]: E0501 12:35:37.206386    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:35:48 minikube kubelet[1614]: E0501 12:35:48.175038    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:36:01 minikube kubelet[1614]: E0501 12:36:01.173979    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:36:13 minikube kubelet[1614]: E0501 12:36:13.174972    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:36:24 minikube kubelet[1614]: E0501 12:36:24.175703    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:36:38 minikube kubelet[1614]: E0501 12:36:38.176116    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:36:51 minikube kubelet[1614]: E0501 12:36:51.176430    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:37:06 minikube kubelet[1614]: E0501 12:37:06.350843    1614 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-k8s-demo:latest"
May 01 12:37:06 minikube kubelet[1614]: E0501 12:37:06.351162    1614 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-k8s-demo:latest"
May 01 12:37:06 minikube kubelet[1614]: E0501 12:37:06.351510    1614 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:springboot-container,Image:springboot-k8s-demo,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k66qk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod springboot-app-6f8c5d596d-kmwjm_default(3e729ca5-a767-4d91-a1ef-f7b207f630d6): ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 01 12:37:06 minikube kubelet[1614]: E0501 12:37:06.353888    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ErrImagePull: \"Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:37:20 minikube kubelet[1614]: E0501 12:37:20.176789    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:37:33 minikube kubelet[1614]: E0501 12:37:33.174987    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:37:44 minikube kubelet[1614]: E0501 12:37:44.185609    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:37:55 minikube kubelet[1614]: E0501 12:37:55.174821    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:38:06 minikube kubelet[1614]: E0501 12:38:06.176139    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:38:20 minikube kubelet[1614]: E0501 12:38:20.174971    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:38:35 minikube kubelet[1614]: E0501 12:38:35.173493    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:38:46 minikube kubelet[1614]: E0501 12:38:46.172226    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:38:57 minikube kubelet[1614]: E0501 12:38:57.175133    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:39:08 minikube kubelet[1614]: E0501 12:39:08.174821    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:39:21 minikube kubelet[1614]: E0501 12:39:21.175633    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:39:32 minikube kubelet[1614]: E0501 12:39:32.175647    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:39:45 minikube kubelet[1614]: E0501 12:39:45.192115    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:40:00 minikube kubelet[1614]: E0501 12:40:00.175647    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:40:11 minikube kubelet[1614]: E0501 12:40:11.176535    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:40:23 minikube kubelet[1614]: E0501 12:40:23.195780    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:40:34 minikube kubelet[1614]: E0501 12:40:34.172594    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:40:47 minikube kubelet[1614]: E0501 12:40:47.172764    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:40:59 minikube kubelet[1614]: E0501 12:40:59.175685    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:41:12 minikube kubelet[1614]: E0501 12:41:12.176165    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:41:24 minikube kubelet[1614]: E0501 12:41:24.175065    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:41:35 minikube kubelet[1614]: E0501 12:41:35.175609    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:41:49 minikube kubelet[1614]: E0501 12:41:49.175176    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:42:02 minikube kubelet[1614]: E0501 12:42:02.175119    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:42:17 minikube kubelet[1614]: E0501 12:42:17.490848    1614 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-k8s-demo:latest"
May 01 12:42:17 minikube kubelet[1614]: E0501 12:42:17.491009    1614 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-k8s-demo:latest"
May 01 12:42:17 minikube kubelet[1614]: E0501 12:42:17.491253    1614 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:springboot-container,Image:springboot-k8s-demo,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k66qk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod springboot-app-6f8c5d596d-kmwjm_default(3e729ca5-a767-4d91-a1ef-f7b207f630d6): ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 01 12:42:17 minikube kubelet[1614]: E0501 12:42:17.492678    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ErrImagePull: \"Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:42:33 minikube kubelet[1614]: E0501 12:42:33.174696    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:42:45 minikube kubelet[1614]: E0501 12:42:45.199251    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:42:59 minikube kubelet[1614]: E0501 12:42:59.174853    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:43:10 minikube kubelet[1614]: E0501 12:43:10.175731    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:43:24 minikube kubelet[1614]: E0501 12:43:24.174679    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:43:38 minikube kubelet[1614]: E0501 12:43:38.176005    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:43:50 minikube kubelet[1614]: E0501 12:43:50.176020    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:44:01 minikube kubelet[1614]: E0501 12:44:01.175443    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:44:13 minikube kubelet[1614]: E0501 12:44:13.175293    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:44:26 minikube kubelet[1614]: E0501 12:44:26.175700    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:44:37 minikube kubelet[1614]: E0501 12:44:37.177344    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:44:49 minikube kubelet[1614]: E0501 12:44:49.177240    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:45:03 minikube kubelet[1614]: E0501 12:45:03.177088    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:45:14 minikube kubelet[1614]: E0501 12:45:14.176964    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:45:27 minikube kubelet[1614]: E0501 12:45:27.174856    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:45:38 minikube kubelet[1614]: E0501 12:45:38.175478    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"
May 01 12:45:53 minikube kubelet[1614]: E0501 12:45:53.174935    1614 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-container\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-k8s-demo\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-k8s-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-6f8c5d596d-kmwjm" podUID="3e729ca5-a767-4d91-a1ef-f7b207f630d6"


==> storage-provisioner [223d68e98001] <==
I0501 12:15:40.473673       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0501 12:15:50.521139       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout


==> storage-provisioner [c9dcd910fa3f] <==
I0501 12:16:04.057468       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0501 12:16:04.112408       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0501 12:16:04.112598       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0501 12:16:21.596408       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0501 12:16:21.596501       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"862bebcb-db95-4dd0-8934-e56bcacb647e", APIVersion:"v1", ResourceVersion:"3505", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_40464e42-5ff5-4afb-a88d-8d3b8e648a2d became leader
I0501 12:16:21.596835       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_40464e42-5ff5-4afb-a88d-8d3b8e648a2d!
I0501 12:16:21.697739       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_40464e42-5ff5-4afb-a88d-8d3b8e648a2d!

